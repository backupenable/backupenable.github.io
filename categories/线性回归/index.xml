<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>线性回归 on 66uan99</title>
        <link>https://backupenable.github.io/categories/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</link>
        <description>Recent content in 线性回归 on 66uan99</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Example Person</copyright>
        <lastBuildDate>Mon, 28 Apr 2025 20:05:36 +0800</lastBuildDate><atom:link href="https://backupenable.github.io/categories/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>机器学习 入门 线性回归(2)</title>
        <link>https://backupenable.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%85%A5%E9%97%A8-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%922/</link>
        <pubDate>Mon, 28 Apr 2025 20:05:36 +0800</pubDate>
        
        <guid>https://backupenable.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%85%A5%E9%97%A8-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%922/</guid>
        <description>&lt;img src="https://backupenable.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%85%A5%E9%97%A8-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%922/%E5%B9%B3%E6%B3%BD%E5%94%AF.jpg" alt="Featured image of post 机器学习 入门 线性回归(2)" /&gt;&lt;h2 id=&#34;34广义线性回归&#34;&gt;3.4广义线性回归
&lt;/h2&gt;&lt;p&gt;一般形式：$y = g^{-1} \left( w^T x + b \right)$&lt;/p&gt;
&lt;p&gt;单调可微的联系函数 (link function)&lt;/p&gt;
&lt;p&gt;令 $g(\cdot) = \ln (\cdot)$ 则得到对数线性回归&lt;/p&gt;
$$\ln y = w^T x + b$$&lt;p&gt;实际上是在用$e^{w^T x + b}$逼近$y$&lt;/p&gt;
&lt;h2 id=&#34;35对率回归&#34;&gt;3.5对率回归
&lt;/h2&gt;&lt;p&gt;线性回归模型产生的实值输出 $z = w^T x + b$&lt;/p&gt;
&lt;p&gt;期望输出 $y \in {0, 1}$&lt;/p&gt;
&lt;p&gt;理想的&amp;quot;单位阶跃函数&amp;quot; (unit-step function)&lt;/p&gt;
$$y = 
\begin{cases} 
0, &amp; z &lt; 0; \\
0.5, &amp; z = 0; \\
1, &amp; z &gt; 0,
\end{cases}$$&lt;p&gt;性质不好，需找&amp;quot;替代函数&amp;quot; (surrogate function)&lt;/p&gt;
&lt;p&gt;常用单调可微、任意阶可导&lt;/p&gt;
$$y = \frac{1}{1 + e^{-z}}$$&lt;p&gt;找 $z$和 $y$的联系函数&lt;/p&gt;
&lt;p&gt;对数几率函数 (logistic function) 简称&amp;quot;对率函数&amp;quot;&lt;/p&gt;
&lt;p&gt;以对率函数为联系函数：$y = \frac{1}{1 + e^{-z}}$&lt;/p&gt;
&lt;p&gt;变为$y = \frac{1}{1 + e^{-(w^T x + b)}}$&lt;/p&gt;
&lt;p&gt;即：$\ln \left( \frac{y}{1 - y} \right) = w^T x + b$&lt;/p&gt;
&lt;p&gt;$\ln \left( \frac{y}{1 - y} \right)$称为几率 (odds)，反映了 $x$ 作为正例的相对可能性（log odds，亦称 &lt;code&gt;logit&lt;/code&gt;）。&lt;/p&gt;
&lt;p&gt;“对数几率回归”（logistic regression）简称“对率回归”&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;无需事先假设数据分布&lt;/li&gt;
&lt;li&gt;可得到“类别”的近似概率预测&lt;/li&gt;
&lt;li&gt;可直接应用现有数值优化算法求取最优解&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;注意：它是分类学习算法！&lt;/p&gt;
&lt;h2 id=&#34;36多分类任务&#34;&gt;3.6多分类任务
&lt;/h2&gt;&lt;h3 id=&#34;一对多one-vs-rest-ovr&#34;&gt;一对多（One-vs-Rest, OvR）
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;原理&lt;/strong&gt;
为每个类别训练一个独立的二分类器，将该类别作为正类，其他所有类别合并作为负类&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;实现步骤&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;假设共有K个类别&lt;/li&gt;
&lt;li&gt;训练K个二分类器（如逻辑回归、SVM等）&lt;/li&gt;
&lt;li&gt;第i个分类器的训练数据：
&lt;ul&gt;
&lt;li&gt;正样本：原始数据中标签为类别i的样本&lt;/li&gt;
&lt;li&gt;负样本：原始数据中标签不为类别i的所有样本&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;预测时：
&lt;ul&gt;
&lt;li&gt;用所有K个分类器分别预测&lt;/li&gt;
&lt;li&gt;选择输出概率/分数最高的类别作为最终预测结果&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;特点&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;优点：只需训练K个分类器，计算效率较高&lt;/li&gt;
&lt;li&gt;缺点：当类别数很多时，每个分类器的负样本会远多于正样本，导致类别不平衡问题&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;一对一one-vs-one-ovo&#34;&gt;一对一（One-vs-One, OvO）
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;原理&lt;/strong&gt;
为每两个类别组合训练一个独立的二分类器，专门区分这两个类别&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;实现步骤&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;假设共有K个类别&lt;/li&gt;
&lt;li&gt;训练K×(K-1)/2个二分类器（如逻辑回归、SVM等）&lt;/li&gt;
&lt;li&gt;每个分类器(i,j)的训练数据：
&lt;ul&gt;
&lt;li&gt;只使用原始数据中标签为i或j的样本&lt;/li&gt;
&lt;li&gt;类别i作为正类，类别j作为负类（或反之）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;预测时：
&lt;ul&gt;
&lt;li&gt;让所有分类器进行预测并记录&amp;quot;投票&amp;quot;&lt;/li&gt;
&lt;li&gt;统计每个类别获得的票数&lt;/li&gt;
&lt;li&gt;选择得票数最多的类别作为最终预测结果&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;特点&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;优点：每个分类器只关注两个类别，训练数据更均衡&lt;/li&gt;
&lt;li&gt;缺点：需要训练O(K²)量级的分类器，当K很大时计算开销显著增加&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>机器学习 入门 线性回归(1)</title>
        <link>https://backupenable.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%85%A5%E9%97%A8-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%921/</link>
        <pubDate>Sun, 27 Apr 2025 18:15:36 +0800</pubDate>
        
        <guid>https://backupenable.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%85%A5%E9%97%A8-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%921/</guid>
        <description>&lt;img src="https://backupenable.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%85%A5%E9%97%A8-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%921/%E5%B9%B3%E6%B3%BD%E5%94%AF.jpg" alt="Featured image of post 机器学习 入门 线性回归(1)" /&gt;&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&#34;31-线性回归&#34;&gt;3.1 线性回归
&lt;/h2&gt;$$ f(x_i) = wx_i + b \quad \text{使得} \quad f(x_i) \simeq y_i $$&lt;p&gt;离散属性的处理：若有&amp;quot;序&amp;quot;（order），则连续化；否则，转化为 $k$ 维向量&lt;/p&gt;
&lt;p&gt;令均方误差最小化，有：&lt;/p&gt;
$$
(w^*, b^*) = \arg\min_{(w, b)} \sum_{i=1}^m (f(x_i) - y_i)^2 = \arg\min_{(w, b)} \sum_{i=1}^m (y_i - wx_i - b)^2
$$$$ E(w, b) = \sum_{i=1}^m (y_i - wx_i - b)^2 $$&lt;p&gt; 进行最小二乘参数估计&lt;/p&gt;
&lt;h2 id=&#34;32-最小二乘解&#34;&gt;3.2 最小二乘解
&lt;/h2&gt;$$ E_{(w,b)} = \sum_{i=1}^m (y_i - wx_i - b)^2 $$&lt;p&gt;分别对 $w$ 和 $b$ 求导：&lt;/p&gt;
$$
\frac{\partial E_{(w,b)}}{\partial w} = 2 \left( w \sum_{i=1}^m x_i^2 - \sum_{i=1}^m (y_i - b)x_i \right)
$$$$
\frac{\partial E_{(w,b)}}{\partial b} = 2 \left( mb - \sum_{i=1}^m (y_i - wx_i) \right)
$$&lt;p&gt;令导数为 0，得到闭式(closed-form)解：&lt;/p&gt;
$$
w = \frac{\sum_{i=1}^m y_i (x_i - \bar{x})}{\sum_{i=1}^m x_i^2 - \frac{1}{m} \left( \sum_{i=1}^m x_i \right)^2} \quad b = \frac{1}{m} \sum_{i=1}^m (y_i - wx_i)
$$&lt;h2 id=&#34;33-多元线性回归&#34;&gt;3.3 多元线性回归
&lt;/h2&gt;&lt;p&gt;同样采用最小二乘法求解，有&lt;/p&gt;
$$ w^* = \arg\min_{w} (y - Xw)^T (y - Xw) $$$$ E_w = (y - Xw)^T (y - Xw) $$&lt;p&gt;，对 $w$ 求导：&lt;/p&gt;
$$
\frac{\partial E_w}{\partial w} = 2X^T (Xw - y)
$$&lt;p&gt;令其为零可得 $w$&lt;/p&gt;
&lt;p&gt;然而，麻烦来了：涉及矩阵求逆！&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;若 $X^T X$ 满秩或正定，则 $$ w^* = (X^T X)^{-1} X^T y $$&lt;/li&gt;
&lt;li&gt;若 $X^T X$ 不满秩，则可解出多个 $w$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;若可解出多个解，可以引入正则化得到唯一解&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
